---
title: "Final Project"
author: "Elysa Strunin, Ekaterina Mikhailova, Shenrui Pan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Metropolis-Hastings

Beta(6,4) is the distribution we need to sample from. The proposal function given is $$\phi_{prop}|\phi_{old}¡«Beta(c*\phi_{old},c*(1 - \phi_{old}))$$ which should be used in the acceptance ratio. We also need to consider the fact that the distribution we are sampling from is not symmetric.

There are four steps for our algorithm.   
1. Choose a starting value.   
2.At iteration t, draw a candidate from the proposal function.   
3.Compute an acceptance ratio r (probability).   
4.Accept the proposal value as the value in iteration t with probability min(r,1). If the proposal value is not accepted, then the value in iteration t should remain the same.   
5.Repeat steps 2-4. 


##c=1
Our code for the part with c=1 is shown as the following.
```{r}
set.seed(1)
#Defination of proposal function
proposalfunction <- function(c, phi_old){   
  rbeta(1, c*phi_old,c*(1-phi_old))
}
#The constant in proposal function
c<-1                                        
# MC algorithm
run_metropolis_MCMC <- function(c, iterations){   
  chain = array(dim = c(iterations+1,1))
  chain[1,] = runif(1)
  for (i in 1:iterations){
    proposal = proposalfunction(c,chain[i,])
    
    probab = dbeta(proposal,6,4)/dbeta(chain[i,1],6,4)*dbeta(chain[i,],c*proposal,c*(1-proposal))/dbeta(proposal,c*chain[i,],c*(1-chain[i,])) # Acceptance Ratio
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

chain=run_metropolis_MCMC(1, 10000)

chain_real <- c(10000,1)
for (i in 1:10000){
  chain_real[i]<- rbeta(1,6,4) #generation a real chain from beta (6,4)
}

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain,type = 'l');abline(h = mean(chain_real),col = 'red'); acf(chain); hist(chain,freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram

```

As can be seen from the chart above, the histogram does not align well with the density function of beta(6,4) (shown as a line).
```{r}
chain_real <- c(10000,1)
for (i in 1:10000){
  chain_real[i]<- rbeta(1,6,4) #generation a real chain from beta (6,4)
}

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain_real,type = 'l');abline(h = mean(chain_real),col = 'red'); acf(chain_real); hist(chain_real,freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram

ks.test(chain, chain_real)
```

The above figure is for the real distribution - namely, the Beta(6,4). The number given is for the Kolmogorov¨CSmirnov test. We can see the two sets of figure do not quite align. The p value we get from the K-S test is quite small, which means that we cannot accept the hypothesis that the two data sets are from the same distribution. 

Next we use "burn in" and thinning to improve the performance of the algorithm. We modify the code to the following.
```{r}
#Definition of proposal function
proposalfunction <- function(c, phi_old){   
  rbeta(1, c*phi_old,c*(1-phi_old))
}
#The constant in proposal function
c<-1                                        
# MC algorithm
run_metropolis_MCMC <- function(c, iterations){   
  chain = array(dim = c(iterations+1,1))
  chain[1,] = runif(1)
  for (i in 1:iterations){
    proposal = proposalfunction(c,chain[i,])
    probab = dbeta(proposal,6,4)/dbeta(chain[i,1],6,4)*dbeta(chain[i,],c*proposal,c*(1-proposal))/dbeta(proposal,c*chain[i,],c*(1-chain[i,])) # Acceptance Ratio, no min here.
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}


chain=run_metropolis_MCMC(1, 10000)

burnIn = 1000

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain[-(1:burnIn),1],type = 'l');abline(h = mean(chain_real),col = 'red'); acf(chain[-(1:burnIn),1]);hist(chain[-(1:burnIn),1],freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram


```

With these modifications implemented, the data more closely follows the probability density function. 

##c=0.1
```{r}
#Defination of proposal function
proposalfunction <- function(c, phi_old){   
  rbeta(1, c*phi_old,c*(1-phi_old))
}
#The constant in proposal function
c<-0.1                                        
# MC algorithm
run_metropolis_MCMC <- function(c, iterations){   
  chain = array(dim = c(iterations+1,1))
  chain[1,] = runif(1)
  for (i in 1:iterations){
    proposal = proposalfunction(c,chain[i,])
    
    probab = dbeta(proposal,6,4)/dbeta(chain[i,1],6,4)*dbeta(chain[i,],c*proposal,c*(1-proposal))/dbeta(proposal,c*chain[i,],c*(1-chain[i,])) # Acceptance Ratio
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

chain=run_metropolis_MCMC(1, 10000)

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain,type = 'l');abline(h = mean(chain_real),col = 'red'); acf(chain); hist(chain,freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram
ks.test(chain, chain_real)
```

##c=2.5
```{r}
#Defination of proposal function
proposalfunction <- function(c, phi_old){   
  rbeta(1, c*phi_old,c*(1-phi_old))
}
#The constant in proposal function
c<-2.5                                        
# MC algorithm
run_metropolis_MCMC <- function(c, iterations){   
  chain = array(dim = c(iterations+1,1))
  chain[1,] = runif(1)
  for (i in 1:iterations){
    proposal = proposalfunction(c,chain[i,])
    
    probab = dbeta(proposal,6,4)/dbeta(chain[i,1],6,4)*dbeta(chain[i,],c*proposal,c*(1-proposal))/dbeta(proposal,c*chain[i,],c*(1-chain[i,])) # Acceptance Ratio
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

chain=run_metropolis_MCMC(1, 10000)

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain,type = 'l');abline(h = mean(chain_real),col = 'red'); acf(chain); hist(chain,freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram
ks.test(chain, chain_real)
```

##c=10
```{r}
#Defination of proposal function
proposalfunction <- function(c, phi_old){   
  rbeta(1, c*phi_old,c*(1-phi_old))
}
#The constant in proposal function
c<-10                                        
# MC algorithm
run_metropolis_MCMC <- function(c, iterations){   
  chain = array(dim = c(iterations+1,1))
  chain[1,] = runif(1)
  for (i in 1:iterations){
    proposal = proposalfunction(c,chain[i,])
    
    probab = dbeta(proposal,6,4)/dbeta(chain[i,1],6,4)*dbeta(chain[i,],c*proposal,c*(1-proposal))/dbeta(proposal,c*chain[i,],c*(1-chain[i,])) # Acceptance Ratio
    if (runif(1) < probab){
      chain[i+1,] = proposal
    }else{
      chain[i+1,] = chain[i,]
    }
  }
  return(chain)
}

chain=run_metropolis_MCMC(1, 10000)

par(mfrow=c(1,3))  #1 row, 3 columns
plot(chain,type = 'l'); abline(h = mean(chain_real),col = 'red');acf(chain); hist(chain,freq = FALSE)  #plot commands
curve(dbeta(x, 6, 4), add=TRUE)  #adding density function of beta(6,4) to histogram
ks.test(chain, chain_real)
```

We conclude that c=10 gives the best estimate. This is based on the tendency of the p-value from the K-S test to be largest when c=10, and the tendency of the histogram to match best with the target density function of Beta(6,4) when c=10.

Sometimes, we found that c=2.5 gave us the best result. Presumably this variation is due to the nature of random variables.

## Gibbs Sampling

We use inverse transform sampling here.   
1. Generate our CDF:
$$F_{X|Y}=\frac{1-e^{-yx}}{1-e^{-yB}}$$
$$F_{Y|X}=\frac{1-e^{-xy}}{1-e^{-xB}}$$
2. Generate two samples from the uniform distribution between 0 and 1. Name them P1 and P2.    
3. For each value P1 and P2 in uniform sample, generate new x and y. The rest of the implementation mirrors the implementation shown in class. Thinning the chain via thin=10 was deemed reasonable here as well.
$$x=-\frac{1}{y}log(1-p~1~+p~1~e^{-yB})$$
$$y=-\frac{1}{x}log(1-p~2~+p~2~e^{-xB})$$

## T=500

```{python}

from math import *
import random
import numpy
#import matplotlib.pyplot as plt
B = 5
T = 500
M = numpy.zeros((T, 3))


def gibbs(T,thin=10):
    x=1
    y=1
    print "Iter  x  y"
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y
        #print i,x,y

gibbs(T)
print "Here is an output of 50 rows from the matrix. For brevity we have not printed the other output matrices in this exercise."
print M[451:500, 0:2]
print "The expectation of X"
print numpy.mean(M[:,1])
#print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.show()

```
![](C:\Users\Veblen\mystuff\histogram_T=500.png)

## T=5000

```{r engine='python', highlight=TRUE}

from math import *
import random
import numpy
#import matplotlib.pyplot as plt
B = 5
T = 5000
M = numpy.zeros((T, 3))


def gibbs(T,thin=10):
    x=1
    y=1
    #print "Iter  x  y"
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y
        #print i,x,y

gibbs(T)
#print M
print "The expectation of X"
print numpy.mean(M[:,1])
#print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.show()


```
![](C:\Users\Veblen\mystuff\histogram_T=5000.png)

## T=50000

```{r engine='python', highlight=TRUE}

from math import *
import random
import numpy
#import matplotlib.pyplot as plt
B = 5
T = 50000
M = numpy.zeros((T, 3))


def gibbs(T,thin=10):
    x=1
    y=1
    #print "Iter  x  y"
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y
        #print i,x,y

gibbs(T)
#print M
print "The expectation of X"
print numpy.mean(M[:,1])
#print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.show()



```
![](C:\Users\Veblen\mystuff\histogram_T=50000.png)

As sample size T increases, the histogram showing X outputs appears less lumpy; it more closely resembles a smooth distribution, as we would expect. Our estimate of the expectation of X appears to increase in accuracy as T increases. As a test we estimated the expectation of Y as well, which we assumed would be the same value (given the symmetry of the CDFs). Both expectations trended toward approximately 1.26 as T increased, which was promising.