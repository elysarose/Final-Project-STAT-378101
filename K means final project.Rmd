---
title: "K means final project"
author: "Elysa Strunin, Ekaterina Mikhailova, Shenrui Pan."
date: '1 ноября 2016 г '
output: html_document
---
We developed K means algorithm wich includes further steps:

Step 1) Randomly assign center locations for each of k clusters

Step 2) Assign each data point to the cluster whose center is closest to that point

Step 3) Recalculate each cluster center as the mean location of all the points assigned to it.

Check to make sure each cluster has at least one point.

Repeat steps 2-3.
```{r}
data(wine, package="rattle")
#Preparing data. Remove column 1 from the analysis
wine <- wine[-1]

#Step 1. Randomly assign 3 epicenters
K_means<-function(x,k){ 
  epicenters <- matrix(0, nrow=3, ncol=13)
  #Sample from each column to obtain an epicenter coordinate
  set.seed(2016)
  for(i in 1:k) {
    for(j in 1:ncol(x)) {
      col_sample <- sample(x[,j], 1)
      epicenters[i,j] <- col_sample
    }
  }
  
#Step 2. Calculate euclidian distances, take the min of each row and assign that column as the cluster; put it in fourth column.
  all_norms <- matrix(0, nrow=nrow(x), ncol=k+2)
  for(i in 1:k) {
    for(j in 1:nrow(x)) {
      e_dist <- dist(rbind(x[j,], epicenters[i,]), method = "euclidean")
      all_norms[j, i] <- e_dist
    }
  }
  for(i in 1:nrow(x)) {
    min_dist_index <- which.min(as.vector(all_norms[i, c(1:k)]))
    all_norms[i,k+1] <- min_dist_index
  }  
#Taking count of iterations. If clusters on iteration t-1 and t are equal then break and print the output.
  count <- 0
  repeat { count <- count + 1
  if  (identical(all_norms[,k+1], all_norms[,k+2])==TRUE) 
  {output <- all_norms[,k+1]
  print(output);
  break
  }else{
#Step 3. Recalculate the epicenters as the means per cluster
    for(i in 1:k) {
      wines_per_cluster <- x[all_norms[,k+1]==i, ]
      column_means_per_cluster <- t(colMeans(wines_per_cluster))
      epicenters[i, ] <- column_means_per_cluster
    }
#Repeating Step 2 and 3: calculating euclidian distances
    for(i in 1:k) {
      for(j in 1:nrow(x)) {
        e_dist <- dist(rbind(x[j,], epicenters[i,]), method = "euclidean")
        all_norms[j, i] <- e_dist
      }
    }

#Take the minimum of each row; assign that column as the cluster on itertaion t; put it in k+2 column
    for(i in 1:nrow(x)) {
      min_dist_index <- which.min(as.vector(all_norms[i, c(1:k)]))
      all_norms[i,k+2] <- min_dist_index
    }  
    
#Checking that each cluster has at least one point,if not - break
    for (i in 1:k){
    if ((length(which(all_norms[,k+2]==i)) > 0) == FALSE ) 
    {
#If one of the clusters has less than one point - print the t-1 cluster assignments, which should have at least one point per cluster
      output <- all_norms[,k+1]
      print(output); break
    } 
    }
#If clusters on iteration t+1 (column k+1) and t (column k+2) are not equal, move the most recent clusters over and loop again
    if ((identical(all_norms[,k+1], all_norms[,k+2]))==FALSE) 
    {
      all_norms[,k+1] <- all_norms[,k+2]
      all_norms[,k+2] <- NA
    } 
  }} 
  return(output)
}

output<-K_means(wine,3)
```

We grouped wine data into three clusters. Now we need to visualize our clustering results. 
```{r}
library(fpc)
plotcluster(wine, output)
```

We can see from the graph that data wine seems to be well separated. We can clearly observe each cluster. Here is some very slight overlapping of clusters, although that might not be representative of the actual arrangement in 13-space.


Accuracy ratio. How well the algorithm's clusters correspond to the three wine types:
In order to obtain percentage of wines clustered correctly (if each cluster represents a wine type):

1) Calculate the sum of: (Type 1 wines matched to type 1 cluster) + (Type 2 wines matched to 
type 2 cluster) + (Type 3 wines matched to type 3 cluster). 

2) Repeat this calculation for the six permutations of the cluster types, since we don't actually know which wine type corresponds to which cluster. (six permutations because in case of 3 clusters we have 3! versions)

3) Across these 6 sums, take the maximum: this is the number of correct matches.

4) Calculate an accuracy ratio: number of correct matches / total observations = 
```{r}
#Recall original data again
data(wine, package="rattle")
wine$Output <- output
correct_matches_fxn<-function(x) {
correct_matches <- max(c(length(which(x[,1]==1 & x[,15]==1)) +
length(which(x[,1]==2 & x[,15]==2)) +
length(which(x[,1]==3 & x[,15]==3)),

length(which(x[,1]==1 & x[,15]==1)) +
length(which(x[,1]==2 & x[,15]==3)) +
length(which(x[,1]==3 & x[,15]==2)),
                 
length(which(x[,1]==1 & x[,15]==2)) +
length(which(x[,1]==2 & x[,15]==1)) +
length(which(x[,1]==3 & x[,15]==3)),
                  
length(which(x[,1]==1 & x[,15]==2)) +
length(which(x[,1]==2 & x[,15]==3)) +
length(which(x[,1]==3 & x[,15]==1)),
                  
length(which(x[,1]==1 & x[,15]==3)) +
length(which(x[,1]==2 & x[,15]==1)) +
length(which(x[,1]==3 & x[,15]==2)),
                  
length(which(x[,1]==1 & x[,15]==3)) +
length(which(x[,1]==2 & x[,15]==2)) +
length(which(x[,1]==3 & x[,15]==1))
))
return(correct_matches)
}
correct_matches<-correct_matches_fxn(wine)
accuracy_ratio <- correct_matches/178
accuracy_ratio
```
70.2% the accuracy ratio id a really good result. 

Now we will use scaled data in order to compare the efficiency and accuracy of the algorithm K means using it on scaled and non-scaled data. 
```{r}
data(wine, package="rattle")
wine_scale <- scale(wine[-1])
output<-K_means(wine_scale,3)
plotcluster(wine_scale, output)
```
We can see from the graph that data wine seems to be well separated. We also can clearly observe each cluster as on the non-scaled data. The main difference is that we do not observe any overlapping here, boundaries are really clear now. 

Let us to compute the accuracy ratio for scaled data:
```{r}
data(wine, package="rattle")
wine$Output <- output
correct_matches<-correct_matches_fxn(wine)
accuracy_ratio <- correct_matches/178
accuracy_ratio
```
The accuracy ratio is about 96% now, which is uch better. This result supports graph, visually the scaled clusters are cleaner than the unscaled data. 




