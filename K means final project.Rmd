---
title: "K Means Final Project"
author: "Elysa Strunin, Ekaterina Mikhailova, Shenrui Pan"
output: html_document
---
We developed a k-means algorithm which includes the following steps:

1. Randomly assign center locations for each of k clusters.

2. Assign each data point to the cluster whose center is closest to that point.

3. Recalculate each cluster center as the mean location of all the points assigned to it. Verify that each cluster has at least one point.    

Repeat steps 2-3 until there is no change in cluster assignments from one iteration to the next.
```{r}
data(wine, package="rattle")
#Preparing data. Remove column 1 from the analysis
wine <- wine[-1]

#Step 1. Randomly assign 3 epicenters
K_means<-function(x,k){ 
  epicenters <- matrix(0, nrow=k, ncol=ncol(x))
  #Sample from each column to obtain an epicenter coordinate
  set.seed(2016)
  for(i in 1:k) {
    for(j in 1:ncol(x)) {
      col_sample <- sample(x[,j], 1)
      epicenters[i,j] <- col_sample
    }
  }
  
#Step 2. Calculate euclidean distances, take the min of each row and assign that column as the cluster; put it in fourth column.
  all_norms <- matrix(0, nrow=nrow(x), ncol=k+2)
  for(i in 1:k) {
    for(j in 1:nrow(x)) {
      e_dist <- dist(rbind(x[j,], epicenters[i,]), method = "euclidean")
      all_norms[j, i] <- e_dist
    }
  }
  for(i in 1:nrow(x)) {
    min_dist_index <- which.min(as.vector(all_norms[i, c(1:k)]))
    all_norms[i,k+1] <- min_dist_index
  }  
#Taking count of iterations. If clusters on iteration t-1 and t are equal then break and print the output.
  count <- 0
  repeat { count <- count + 1
  if  (identical(all_norms[,k+1], all_norms[,k+2])==TRUE) 
  {output <- all_norms[,k+1]
  print(output);
  break
  }else{
#Step 3. Recalculate the epicenters as the means per cluster
    for(i in 1:k) {
      wines_per_cluster <- x[all_norms[,k+1]==i, ]
      column_means_per_cluster <- t(colMeans(wines_per_cluster))
      epicenters[i, ] <- column_means_per_cluster
    }
#Repeating Step 2 and 3: calculating euclidean distances
    for(i in 1:k) {
      for(j in 1:nrow(x)) {
        e_dist <- dist(rbind(x[j,], epicenters[i,]), method = "euclidean")
        all_norms[j, i] <- e_dist
      }
    }

#Take the minimum of each row; assign that column as the cluster on itertaion t; put it in k+2 column
    for(i in 1:nrow(x)) {
      min_dist_index <- which.min(as.vector(all_norms[i, c(1:k)]))
      all_norms[i,k+2] <- min_dist_index
    }  
    
#Checking that each cluster has at least one point,if not - break
    for (i in 1:k){
    if ((length(which(all_norms[,k+2]==i)) > 0) == FALSE ) 
    {
#If one of the clusters has less than one point - print the t-1 cluster assignments, which should have at least one point per cluster
      output <- all_norms[,k+1]
      print(output); break
    } 
    }
#If clusters on iteration t+1 (column k+1) and t (column k+2) are not equal, move the most recent clusters over and loop again
    if ((identical(all_norms[,k+1], all_norms[,k+2]))==FALSE) 
    {
      all_norms[,k+1] <- all_norms[,k+2]
      all_norms[,k+2] <- NA
    } 
  }} 
  return(output)
}

output<-K_means(wine,3)
```

We grouped wine data into three clusters. Now we need to visualize our clustering results. 
```{r}
library(fpc)
plotcluster(wine, output, main="Clusters: Unscaled Wine Data")
```

We can see from the plot that the wine data seems to be well separated. We can clearly observe each cluster, although there is some very slight overlapping of clusters.


Our calculation of an accuracy ratio signals how well the algorithm's clusters correspond to the three wine types. Here we obtain the percentage of wines clustered correctly (if each cluster represents a wine type):

1) Calculate the sum of: (Type 1 wines matched to type 1 cluster) + (Type 2 wines matched to 
type 2 cluster) + (Type 3 wines matched to type 3 cluster). 

2) Repeat this calculation for the 6 permutations of the cluster types, since we don't actually know which wine type corresponds to which cluster. (Six permutations because in the case of 3 clusters we have 3! versions.)

3) Across these 6 sums, take the maximum: this is the number of correct matches.

4) Calculate an accuracy ratio: number of correct matches / total observations = 
```{r}
#Recall original data again
data(wine, package="rattle")
wine$Output <- output
correct_matches_fxn<-function(x) {
correct_matches <- max(c(length(which(x[,1]==1 & x[,15]==1)) +
length(which(x[,1]==2 & x[,15]==2)) +
length(which(x[,1]==3 & x[,15]==3)),

length(which(x[,1]==1 & x[,15]==1)) +
length(which(x[,1]==2 & x[,15]==3)) +
length(which(x[,1]==3 & x[,15]==2)),
                 
length(which(x[,1]==1 & x[,15]==2)) +
length(which(x[,1]==2 & x[,15]==1)) +
length(which(x[,1]==3 & x[,15]==3)),
                  
length(which(x[,1]==1 & x[,15]==2)) +
length(which(x[,1]==2 & x[,15]==3)) +
length(which(x[,1]==3 & x[,15]==1)),
                  
length(which(x[,1]==1 & x[,15]==3)) +
length(which(x[,1]==2 & x[,15]==1)) +
length(which(x[,1]==3 & x[,15]==2)),
                  
length(which(x[,1]==1 & x[,15]==3)) +
length(which(x[,1]==2 & x[,15]==2)) +
length(which(x[,1]==3 & x[,15]==1))
))
return(correct_matches)
}
correct_matches<-correct_matches_fxn(wine)
accuracy_ratio <- correct_matches/178
accuracy_ratio
```
At 70.2%, the accuracy ratio is a very good result. 

Now we will use scaled data in order to compare the efficiency and accuracy of the K means algorithm on scaled versus unscaled data. scale() is a function that centers and/or scales the columns of a numeric matrix. For example, large values of the unscaled data, such as the Magnesium and Proline values, will be scaled to a magnitude more comparable to the other data points.
 
```{r}
data(wine, package="rattle")
wine_scale <- scale(wine[-1])
output<-K_means(wine_scale,3)
plotcluster(wine_scale, output, main="Clusters: Scaled Wine Data")
```
         
We can see from the graph that the scaled wine clusters are very well-separated. We can clearly observe each cluster. The main difference from the unscaled data is that we do not observe any overlapping here; boundaries are very clear now. 

Let us compute the accuracy ratio for scaled data:
```{r}
data(wine, package="rattle")
wine$Output <- output
correct_matches<-correct_matches_fxn(wine)
accuracy_ratio <- correct_matches/178
accuracy_ratio
```
The accuracy ratio is about 96% now, which is much better than the unscaled data. This result corroborates the graph; visually the scaled clusters are cleaner than the unscaled clusters. Scaling the data has reduced the in-cluster variation that was caused more by magnitude than anything else.


Now we look at the iris data. 
```{r}
data(iris)
iris<-iris[-5]
output<-K_means(iris,3)
```
We grouped iris data into three clusters. Now we need to visualize our clustering results. 
```{r}
library(fpc)
plotcluster(iris, output, main="Clusters: Unscaled Iris Data")
```

We can see from the graph that iris data separated poorly. Our clustering function defined only 2 clusters on unscaled data. 


Accuracy ratio:
```{r}
data(iris)
iris[,5]<-as.numeric(iris[,5])
iris$Output <- output
correct_matches_fxn<-function(x) {
correct_matches <- max(c(
length(which(x[,5]==1 & x[,6]==1)) +
length(which(x[,5]==2 & x[,6]==2)) +
length(which(x[,5]==3 & x[,6]==3)),

length(which(x[,5]==1 & x[,6]==1)) +
length(which(x[,5]==2 & x[,6]==3)) +
length(which(x[,5]==3 & x[,6]==2)),
                 
length(which(x[,5]==1 & x[,6]==2)) +
length(which(x[,5]==2 & x[,6]==1)) +
length(which(x[,5]==3 & x[,6]==3)),
                  
length(which(x[,5]==1 & x[,6]==2)) +
length(which(x[,5]==2 & x[,6]==3)) +
length(which(x[,5]==3 & x[,6]==1)),
                  
length(which(x[,5]==1 & x[,6]==3)) +
length(which(x[,5]==2 & x[,6]==1)) +
length(which(x[,5]==3 & x[,6]==2)),
                  
length(which(x[,5]==1 & x[,6]==3)) +
length(which(x[,5]==2 & x[,6]==2)) +
length(which(x[,5]==3 & x[,6]==1))
))
return(correct_matches)
}
correct_matches<-correct_matches_fxn(iris)
accuracy_ratio <- correct_matches/nrow(iris)
accuracy_ratio
```
66.6% is the accuracy ratio. Although this is a good result, the result from the graph shows us that clustering has not been successful. 

Now we will use scaled data in order to compare the efficiency and accuracy of the k-means algorithm by using it on scaled and unscaled data. 
```{r}
data(iris)
iris_scale <- scale(iris[-5])
output<-K_means(iris_scale,3)
plotcluster(iris_scale, output, main="Clusters: Scaled Iris Data")
```

We can see from the graph that scaled iris data seems to be well separated in comparison with non-scaled data. We can clearly observe three clusters.  

Let us compute the accuracy ratio for scaled data:
```{r}
data(iris)
iris[,5]<-as.numeric(iris[,5])
iris$Output <- output
correct_matches<-correct_matches_fxn(iris)
accuracy_ratio <- correct_matches/nrow(iris)
accuracy_ratio
```
The accuracy ratio is about 85.3% now, which is much better than the unscaled data. This statistic supports the chart; visually the scaled clusters are much more distinct than the unscaled clusters. When we scale the iris data, k-means is effective in classifying this dataset.

