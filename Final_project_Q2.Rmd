---
title: "Final Project"
author: "Elysa Strunin, Ekaterina Mikhailova, Shenrui Pan"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Gibbs sampling

We use inverse transform sampling here.   
1. Generate our CDF:
$$F_{X|Y}=\frac{1-e^{-yx}}{1-e^{-yB}}$$
$$F_{Y|X}=\frac{1-e^{-xy}}{1-e^{-xB}}$$
2. Generate two samples from the uniform distribution between 0 and 1. Name them $P_{1}$ and $P_{1}$.    
3. For each value $P_{1}$ and $P_{2}$ in uniform sample, generate new x and y. The rest of the implementation mirrors the implementation shown in class. Thinning the chain via thin=10 was deemed reasonable here as well.
$$x=-\frac{1}{y}log(1-p_{1}~+p_{1}~e^{-yB})$$
$$y=-\frac{1}{x}log(1-p_{2}+p_{2}~e^{-xB})$$

## T=500
In the following codes, the reason we commented all our codes about matplotlib is that we can not run this code in r Markdown although we can run them in the terminal. The figures we pasted here were gotten from the outcomes of terminal. 
```{python}
from math import *
import random
import numpy
#import matplotlib.pyplot as plt

B = 5
T = 500
M = numpy.zeros((T, 3))

def gibbs(T,thin=10):
    x=1
    y=1
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y

gibbs(T)
print "The expectation of X"
print numpy.mean(M[:,1])
print "The expectation of Y"
print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.text(2.5,180,'T = 500')
#plt.xlabel('X value')
#plt.ylabel('Frequency')
#plt.show()


```
![](C:\Users\Veblen\mystuff\histogram_T=500.png)

## T=5000

```{r engine='python', highlight=TRUE}
from math import *
import random
import numpy
#import matplotlib.pyplot as plt

B = 5
T = 5000
M = numpy.zeros((T, 3))

def gibbs(T,thin=10):
    x=1
    y=1
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y

gibbs(T)
print "The expectation of X"
print numpy.mean(M[:,1])
print "The expectation of Y"
print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.text(2.5,1800,'T = 5000')
#plt.xlabel('X value')
#plt.ylabel('Frequency')
#plt.show()


```
![](C:\Users\Veblen\mystuff\histogram_T=5000.png)

## T=50000

```{r engine='python', highlight=TRUE}
from math import *
import random
import numpy
#import matplotlib.pyplot as plt

B = 5
T = 50000
M = numpy.zeros((T, 3))

def gibbs(T,thin=10):
    x=1
    y=1
    for i in range(T):
        for j in range(thin):
            p1= random.uniform(0,1)
            x=-1/y*log(1-p1+p1*exp(-y*B))
            p2= random.uniform(0,1)
            y=-1/x*log(1-p2+p2*exp(-x*B))
        M[i,0] = i
        M[i,1] = x
        M[i,2] = y

gibbs(T)
print "The expectation of X"
print numpy.mean(M[:,1])
print "The expectation of Y"
print numpy.mean(M[:,2])
#plt.hist(M[:,1])
#plt.text(2.5,18000,'T = 50000')
#plt.xlabel('X value')
#plt.ylabel('Frequency')
#plt.show()



```
![](C:\Users\Veblen\mystuff\histogram_T=50000.png)

As sample size T increases, the histogram showing X outputs appears less lumpy; it more closely resembles a smooth distribution, as we would expect. Our estimate of the expectation of X appears to increase in accuracy as T increases. As a test we estimated the expectation of Y as well, which we assumed would be the same value (given the symmetry of the CDFs). Both expectations trended toward approximately 1.26 as T increased, which was promising.

